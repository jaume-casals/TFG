\section{Models destacats}
De la comparació dels models a l'apartat \ref{sec:comparacio-models} n'hi ha uns quants que han destacat per sobre dels altres. Aquests models han continuat per les proves de selecció i han sigut sotmesos a més testos i a afinaments per quantificar el seu rendiment. D'aquesta manera, es pot escollir el model més adequat pel projecte de manera acurada.

A causa del retard en l'arribada de les dades, aquest apartat es divideix en dues seccions diferents. La primera secció presenta els models comparats utilitzant dades de prova utilitzant el hardware disponible a l'oficina en el moment el qual només disposa de CPU. La segona secció avalua uns nous models amb les dades reals, aprofitant el portàtil equipat amb una GPU procedent de l'Agència.

\subsection{Comparació de models amb dades sintètiques}
\subsubsection{Mistral 7B (Instruct)}
\textit{Mistral 7B Instruct} \cite{mistral} és un model del llenguatge natural que està afinat a partir del model \textit{Mistral 7B}. Aquest reentrenament es va realitzar, segons expliquen els creadors, per demostrar la facilitat que té el model original per adaptar-se a qualsevol tasca. Aquesta és una de les raons que va cridar molt l'atenció aquest model.

Un altre motiu pel qual aquest model és tan interessant és que afirma ser el millor model de la seva mida. Això ha sigut comprovar comparant aquest model amb els més populars de la seva categoria en els \textit{datasets} més populars.

El problema que té aquest model és que té massa paràmetres i tardaria massa temps a ser afinat correctament. De fet, el model sencer no es pot fer servir a les màquines disponibles, havent de recórrer a la versió quantificada del model per poder fer la inferència. 

Aquest model s'ha executat més concretament utilitzant el programa \textit{localGPT} \cite{localgpt}. El programa permet, en essència, parlar a un assistent sobre un document indicat. \textit{LocalGPT} primer ingereix un document i després deixa fer preguntes sobre el document, tenint l'habilitat de comprendre'l, llegir-lo i extreure informació per contestar tots els dubtes. A més a més, pot recordar el context de la conversació, el que significa que es poden fer preguntes encadenades fent referència a les anteriors respostes.

En les proves, ha donat molt bons resultats, encertant en gairebé totes les preguntes. El seu temps d'execució és bastant lent, però acceptable pels objectius que es plantegen. Finalment, aquest model s'ha descartat, ja que no es pot assegurar que les màquines de l'\textbf{Agència} tinguin la capacitat per executar aquest model.


\subsubsection{Llama 2}
\textit{Llama 2} \cite{llama} és un model NLP de codi obert desenvolupat per \textit{Meta}. Aquest model va ser publicat afirmant ser el millor model durant el seu temps en moltes proves de rendiment. Té $7000M$ paràmetres, fent impossible la seva execució localment amb els dispositius disponibles. Per aquest motiu, s'executa la seva versió quantificada. Dona resultats acceptables, però lluny de ser la solució del problema i, molt inferior comparat amb \textit{Mistral 7B}, el seu principal competidor.

Aquest model també és compatible amb el programa \textit{LocalGPT} \cite{localgpt} i, és per això i per la seva mida similar que és tan comparat amb el model \textit{Mistral 7B}. \textit{Llama 2} és el model que està seleccionat per defecte al programa \textit{localGPT}, i per aquesta raó va ser el primer a ser provat. En veure la millora significativa comparat amb el seu model competidor, va ser ràpidament substituït per aquesta versió millorada seva.


\subsubsection{Flan-T5-Base}
El model \textit{Flan-T5-Base} \cite{flan-t5} és un model de processament del llenguatge natural que presenta millores en tots els aspectes respecte al seu model previ: \textit{T5 Base}. Els seus desenvolupadors van escriure: 
\begin{quote}
``Amb T5, proposem reformular totes les tasques de NLP en un format unificat de text a text on l'entrada i la sortida són sempre cadenes de text, a diferència dels models d'estil BERT que només poden donar com a sortida una etiqueta de classe o un tram de l'entrada. El nostre marc de text a text ens permet utilitzar el mateix model, funció de pèrdua i hiperparàmetres en qualsevol tasca NLP.''
\end{quote}

Un punt important d'aquest model és que està entrenat amb 60 llenguatges. Això significa que té una comprensió molt gran de tots els texts que se li puguin donar. A la vegada, no interessa tant que entengui gaires idiomes, ja que el format d'entrada estarà molt centrat en un idioma.

Durant els experiments, ha demostrat una capacitat de comprensió del llenguatge i del tiquet. El principal problema és que, en la majoria dels casos, no compleix correctament l'objectiu especificat, a excepció d'una tasca en concret. Gràcies al potencial que s'ha vist en el model, es continua el procés, afinant el model amb una tasca de NER.

L'afinament ha finalitzat satisfactòriament aportant un \textit{rouge1} de $0,96$. Fent proves amb inferència s'ha vist que aquest model contesta correctament a la majoria, però ha fallat sobretot perquè repeteix més d'una vegada les respostes.

\subsubsection{Flan-T5-Small}
\textit{Flan-T5-Small} \cite{flan-t5} és el germà més petit de la família \textit{Flan-T5}. Tenint això en compte, és lògic pensar que donaria un pitjor rendiment que els altres models. El raonament darrere de la decisió de provar aquest model a fons és aconseguir un model que seria més ràpid, tant en entrenament com en inferència, a canvi de perdre poca precisió en les respostes.

Aquest model, tal com s'havia vist, té la capacitat de formular frases senzilles, però en una primera instància no ha sigut capaç de respondre cap pregunta correctament. Després de l'afinament, s'ha comprovat que no ha assolit trobar els patrons més bàsics de les dades, donant així un resultat negatiu i descartant el model per futurs passos.

\subsubsection{Flan-T5-Base (LaMini)}
Repetint el mateix procés que amb la seva versió inferior, \textit{Flan-T5-Base (LaMini)} \cite{flan-t5} \cite{lamini} és la versió afinada del model \textit{Flan-T5-Base} amb el \textit{dataset} per a seguir instruccions \textit{LaMini}. A causa dels bons resultats reportats pels mateixos creadors, amb un augment del rendiment en totes les proves comparatives, es decideix afinar aquest model, en cerca d'un rendiment superior als resultats obtinguts al \textit{Flan-T5-Base}. 

S'ha de destacar que aquest model en les seves primeres proves no va destacar per la seva capacitat pràctica de resoldre els reptes plantejats. De fet, va ser més aviat les promeses teòriques que van impulsar a continuar amb el desenvolupament d'aquest model, ja que, en un principi va demostrar un rendiment similar o lleugerament inferior que el model del qual parteix, \textit{Flan-T5-Base}.

\subsubsection{Flan-T5-Small (LaMini)}
El model \textit{Flan-T5-Small (LaMini)} \cite{flan-t5} \cite{lamini} és una versió afinada del model \textit{Flan-T5-Small} amb el \textit{dataset} per a seguir instruccions \textit{LaMini}. Amb aquest afinament, el model és capaç de millorar respecte al model del qual ha partit. Els creadors afirmen que té el millor rendiment general donat la seva mida.

Malauradament, no s'ha notat cap millora respecte al model anterior. No només no ha sigut capaç de generar text relacionat amb el text donat sinó que no ha contestat a les preguntes seguint el format amb el qual ha sigut entrenat. Deguts aquests motius, i veient que és el model més potent de la seva categoria, ha quedat clar que és necessari més paràmetres per poder completar aquesta tasca correctament. El descens sorprenent del rendiment observat durant la prova del model \textit{Flan-T5-Small (LaMini)} per a la resolució d'una tasca específica, planteja diverses causes potencials per a aquest resultat:

\begin{itemize}
  \item \textbf{Qualitat del model:} El conjunt de dades \textit{LaMini} pot tenir característiques perjudicials per al rendiment a la tasca entrenada. Atès que el \textit{LaMini dataset} ha sigut generat sintèticament, és possible que no capturi amb precisió la complexitat i els matisos dels escenaris del món real per a dur a terme correctament les proves en qüestió. La naturalesa sintètica de les dades podria introduir biaixos o patrons poc realistes que afectin negativament la capacitat del model per generalitzar ara la tasca específica.
  \item \textbf{Rendiments decreixents a l'afinament:} L'ajust repetitiu del model pot haver resultat en un ajust excessiu, disminuint-ne l'adaptabilitat durant les proves. Els múltiples cicles d'afinament podrien haver trobat inicialment els patrons més rellevants, de manera que quan s'ha ajustat per última vegada, oferiria millores limitades i introduiria soroll potencialment. Tot i destacar en tasques generals de seguiment d'instruccions, el conjunt de dades \textit{LaMini} pot no ajustar-se als requisits específics de la tasca.
\end{itemize}

% \subsubsection{Flan-T5 Large}


\subsection{Comparació de models amb dades reals}

\subsubsection{FLOR-6.3B-Instructed}
A causa del fet que el camp de la IA generativa és un camp que evoluciona molt ràpidament, encara que se seleccioni un model pel seu entrenament, es continua observant els diferents models que van sortir. Com s'explica en l'apartat de riscos potencials \ref{ssec:abast-riscos}, es va demanar permisos per la instal·lació de WSL (Subsistema de Windows per Linux) i va trigar un mes fins a donar permisos.

A causa d'això, s'ha buscat models alternatius que es puguin entrenar amb llibreries comunes disponibles a Windows. Aquesta cerca és bastant peculiar, ja que requereix els pesos del model seleccionat càpiga en una GPU de 8 GB i a més a més que pugui entendre el català, castellà i, en certa manera, l'anglès. A part del model \textit{Flan-T5}, s'ha trobat el model \textit{FLOR}, desenvolupat pel projecte AINA. Aquest model té diverses mides que són de 780M, 1.3B i 6.3B.

S'ha optat per entrenar el model FLOR 1.3B per l'extracció dels camps que s'han definit en el projecte. Es creu que aquest model hauria de comportar-se millor que el QWEN-1.5, ja que se centra específicament en els 3 idiomes que són rellevants pel problema. En canvi, el QWEN-1.5 té molts més idiomes i podria ser pitjor en idiomes com el català.

Tal com s'havia comprovat a les primeres proves, el model no entén el format de sortida, el qual és crucial per poder completar el procés d'anonimització i emmagatzematge. Fins i tot després d'un entrenament de 10 hores no arriba als estàndards mínims requerits.

\subsubsection{Qwen1.5-7B}
El model \textit{Qwen1.5} s'ha provat després de dur a terme les proves d'entrenament en la màquina proporcionada per l'Agència. Aquesta màquina té 8 GB de GPU, la qual cosa és una gran limitació per entrenar models LLM. Per tant, després de fer proves amb els models anteriors, es va observar que el model \textit{Mistral} és un model massa gran per a poder-lo entrenar en la GPU d'aquesta màquina. Els models de la família \textit{Flan-T5} tampoc tenen suport GPTQ, cosa que ens permet escalar els pesos dels models per poder-los entrenar en màquines amb menys recursos. Per tant, el màxim que es va poder entrenar va ser el model \textit{Flan-T5} utilitzant els adaptadors LoRa i PEFT. Aquest model té 250M paràmetres, la qual cosa va demostrar no ser suficient per a aquest projecte.

Amb el ràpid creixement d'aquest camp, nous models apareixen cada setmana. Entre aquests models, es va trobar el model \textit{Qwen1.5}, un model que s'assembla al \textit{Flan-T5} en la seva filosofia multilingüe i multitasca, però promet resultats molt millors. Aquest model té 6 versions: 0.5B, 1.8B, 4B, 7B, 14B i 72B. A més a més, té suport per molts idiomes, incloent-hi el català. Després de fer proves, es creu que la versió 7B quantitzada i entrenat amb \textit{QLoRA} pot ser idònia per entrenar en aquesta màquina. La raó per la qual es van poder entrenar models molt més grans que el \textit{Flan-T5} base és perquè té versions GPTQ, cosa que el \textit{Flan-T5} no disposa.

Finalment, cal esmentar que aquest model permet un context de fins a trenta-dos mil tokens, la qual cosa és un avantatge, ja que podrem entrenar el model amb el tiquet sencer. A diferència del T5, on hauríem creat el model per article del tiquet, cosa que hauria provocat la pèrdua del context global del tiquet.

\subsection{Justificació de la tria}

L'entrenament del model final va començar amb \textit{FLOR 6.3B}, ja que era el millor candidat pel fet d'estar entrenat en els 3 idiomes que es requereixen en aquest projecte. Una conseqüència positiva de només estar centrat en aquests tres idiomes és que el model és lleugerament inferior en mida que el model original.

L'entrenament del model es va realitzar durant unes 10 hores pel fet que la versió GPTQ no estava disponible. Addicionalment, cal mencionar que les dades es van preparar de forma que l'output sigui un JSON. Això no obstant, a l'hora d'avaluar els resultats del model entrenat, el model només continuava el tiquet de l'input amb dades addicionals que no existeixen en el tiquet original. Per tant, aquest model no va treure els resultats i la qualitat que s'esperava. En conseqüència, es va decidir descartar aquest model, ja que més es valora és que els resultats surtin de forma organitzada com un JSON.

Pel fet que el \textit{FLOR 6.3B} no ha demostrat uns resultats molt aptes per aquest projecte, es va decidir centrar l'atenció en l'últim model \textit{Qwen1.5}. Es va entrenar el model de 7B i l'entrenament d'aquest model va ser més ràpid que els altres gràcies al fet que s'utilitzava la versió quantitzada i entrenant només un nombre petit de paràmetres. En analitzar els resultats, s'observa que el model és capaç de treure un JSON en bon format i a més és capaç de treure algun dels camps que hi ha en el tiquet. És per això que es decideix escollir aquest model per l'entrenament i futura implementació al resultat final.

