\section{Aprenentatge autònom}

\subsection{Models d'aprenentatge autònom}
L'aprenentatge autònom (\textit{Machine learning} en anglès) és un subcamp de la intel·ligència artificial que permet als ordinadors aprendre els patrons i regles implícites en grans conjunts de dades. Aquests després, es poden fer sevir per fer prediccions o decisions i millorar-ne el rendiment amb l'experiència sense necessitat de programació explícita. Les solucions d'aprenentatge autònom s'apliquen àmpliament a diversos sectors, com ara el comerç electrònic, la sanitat, les finances o la indústria.

A continuació es mostra com hi ha diversos tipus d'algorismes d'aprenentatge autònom segons el resultat desitjat i el tipus de dades que es disposin.

\subsubsection{Classificació segons la naturalesa de les dades}
Segons la naturalesa de les dades, l'aprenentatge autònom es pot dividir en tres tipus principals: 
\begin{itemize}
    \item \textbf{Supervisat:} Aquests algoritmes s'entrenen amb conjunts de dades etiquetades que contenen les variables d'entrada i sortida. L'objectiu principal és aprendre una funció que relacioni les dades d'entrada amb les de sortida i, a continuació, aplicar-la per fer prediccions sobre dades noves o desconegudes. La regressió, classificació i detecció d'anomalies són exemples d'aprenentatge autònom supervisat.
    \item \textbf{No supervisat:} En aquest tipus, els algoritmes s'entrenen amb conjunts de dades no etiquetades que només contenen les variables d'entrada. El seu objectiu és descobrir l'estructura o la distribució subjacent de les dades per agrupar-les o segmentar-les en categories significatives. Alguns exemples d'aprenentatge autònom no supervisat són l'agrupació i la reducció de la dimensionalitat.
    \item \textbf{Per reforç:} L'aprenentatge autònom per reforç implica algorismes que no depenen de conjunts de dades externes, sinó que aprenen de les mateixes accions i de la retroalimentació rebuda de l'entorn. L'objectiu és determinar la política o estratègia òptima que maximitzi la recompensa o minimitzi el cost al llarg del temps. Normalment, s'utilitza en situacions on hi ha un món virtual i un agent que el pugui explorar.
\end{itemize}

\subsubsection{Classificació segons la tasca a resoldre}
Segons la tasca a resoldre, l'aprenentatge autònom es pot dividir principalment en quatre tipus, encara que hi ha altres categories i moltes subcategories dins de cada tasca. 
\begin{itemize}
    \item \textbf{Multimodal:} Són les tasques entre les quals s'inclou el processament i integració de diverses modalitats, incloent-hi imatges, àudio, text i més. El propòsit és establir una representació cohesiva de les dades independentment del seu format i s'usa en una àmplia gamma d'aplicacions, com creació i edició d'imatges, extracció d'informació i descripció d'imatges.
    \item \textbf{Visió per Computador:} Aquesta tasca consisteix a analitzar i comprendre les dades visuals, tals com imatges i vídeos amb l'objectiu d'extreure informació o característiques de les dades per utilitzar-les en detecció d'imatges, el reconeixement de cares o la segmentació d'escenes, entre altres.
    \item \textbf{Processament del Llenguatge Natural:} Consisteix en l'anàlisi i generació del llenguatge natural, és a dir, text. El seu objectiu és entendre el significat i propòsit de les paraules que rep. Les dades llavors poden ser aplicades a altres tasques com el resum de textos, la traducció o la generació de text. Aquesta és la tasca dels models que es fan servir en aquest projecte, i en l'apartat \ref{ssec:definicio_NLP} es parlarà en més en profunditat.
    \item \textbf{Àudio:} El processament d'àudio s'ocupa de la manipulació i generació d'àudio, tant veu, com música o efectes de soroll. L'objectiu és extreure informació o característiques de la pista d'àudio pel reconeixement de veu, la generació de música o la recomanació de música. 
\end{itemize}

\subsection{Deep learning}
El \textit{deep learning} (aprenentatge profund en català) és un subcamp del \textit{machine learning} que aprofita les xarxes neuronals per analitzar dades. 

Les xarxes neuronals consisteixen en capes de neurones artificials que adquireixen patrons i característiques complexos a partir de les dades d'entrada. Fa servir aquestes xarxes neuronals amb múltiples capes per extreure'n característiques i patrons complexos de les dades. Cada capa neuronal obté informació de la capa anterior, fa càlculs i transmet el resultat a la següent capa. 

Els models de \textit{deep learning} poden aprendre de grans quantitats de dades i assolir una precisió i eficiència elevades. Destaquen sobretot, perquè són capaços de fer tasques complexes que els algoritmes d'aprenentatge autònom tradicionals consideren difícils, com ara la visió per ordinador i el processament del llenguatge natural. Tot i això, l'aprenentatge profund requereix més recursos i temps per l'entrenament i execució que el \textit{machine learning}.


\subsubsection{Fine-Tuning}
El \textit{Fine-Tuning} o afinament en el context del \textit{deep learning} és el procés d'ajustar una xarxa neuronal preentrenada per a una tasca específica. Aquest enfocament evita entrenar una xarxa neuronal des de zero, adaptant un model que ja ha estat entrenat en un conjunt de dades ampli i general. Aquest enfocament és especialment útil quan el conjunt de dades pel \textit{Fine-Tuning} és més petit i pot no ser adequat per entrenar un model d'alt rendiment des del principi.

La tècnica consisteix a ajustar els pesos i paràmetres del model preentrenat durant l'entrenament al conjunt de dades específiques. Aquest procés permet que el model aprengui matisos i informació específics de la tasca, alhora que conserva els valuosos coneixements adquirits a la fase de preentrenament.


\subsubsection{Few-shot learning}
El \textit{Few-shot learning} o aprenentatge d'uns pocs cops, inclòs el \textit{One-shot learning} o aprenentatge d'un sol cop, aborda escenaris en què un model s'entrena per fer prediccions basant-se en un nombre limitat d'exemples. En els entorns tradicionals, els models solen requerir grans quantitats de dades d'entrenament per aconseguir un rendiment satisfactori. En contraposició, en el \textit{Few-shot learning}, l'objectiu és que un model funcioni bé fins i tot quan se li presenten només unes poques instàncies d'exemple durant la inferència, evitant així un reentrenament.

El funcionament és tan senzill com exposar un exemple o diversos exemples del problema ja solucionat amb la seva resposta i fer contestar al model en el mateix missatge un problema similar. El raonament és que el model entén el que s'espera que hagi de retornar i produeix així una inferència molt més satisfactòria.

\subsubsection{Avaluació dels models}
Per avaluar i millorar l'entrenament d'un model, s'utilitzen certes mètriques que permeten quantificar com s'assembla la generació del model amb la resposta correcta de referència. S'ha fet ús de les següents mètriques:
\begin{itemize}
    \item \textit{Train\_Loss}: Error durant l'entrenament. Calculat automàticament per la funció \textit{Trainer} de la llibreria \texttt{Transformers}. La funció pel còmput d'aquest error s'escull automàticament i depèn de la tasca amb la qual s'està entrenant, en aquest cas, fa servir la funció de \texttt{CrossEntropyLoss} (Error d'entropia creuada).
    \item \textit{Val\_Loss} Error durant la validació. Es calcula també automàticament i fa servir la funció de \texttt{CrossEntropyLoss}.
    \item \textit{ROUGE-1}: Mesura la superposició d'unigrames (cada paraula) entre el text generat i el text de referència.
    \item \textit{ROUGE-2}: Mesura el solapament d'unigrames (dues paraules seguides) entre el text generat i el text de referència.
    \item \textit{ROUGE-L}: Mesura la subseqüència comuna més llarga (LCS) de les paraules entre el text generat i el text de referència.
    \item \textit{ROUGE-Lsum}: Mesura la LCS de les paraules entre el text generat i el text de referència, però també inclou un factor de normalització de longitud que premia les LCS més llargues.
\end{itemize}

\subsection{Processament del Llenguatge Natural} \label{ssec:definicio_NLP}
El processament del llenguatge natural (\textit{NLP} per les seves sigles en anglès) es refereix a la disciplina de dissenyar màquines capaçes de comprendre el llenguatge humà, o dades semblants al llenguatge humà, de la manera com s'escriu, es parla i s'organitza. Els models d'aprenentatge profund poden aprendre a representar el llenguatge natural d'una manera significativa i utilitzar aquestes representacions per realitzar diverses tasques de \textit{NLP} \cite{Hugging-Face}. Per cadascuna d'elles es descriu i s'indica si és adequada pel projecte que es desenvolupa.

\begin{itemize}
    \item \textbf{Classificació de text:} És la tasca d'assignar una etiqueta o classe a un text donat. Aquesta etiqueta normalment es proporciona amb la probabilitat de que el text tingui certa etiqueta. Alguns dels seus usos són l'anàlisi de sentiments, la inferència del llenguatge natural i comprovar la correctesa del llenguatge. Aquesta tasca no es pot aplicar de manera directa al cas d'ús d'aquest projecte, però es va intentar fer servir per l'eliminació de les signatures dels tiquets.
    \item \textbf{Resum de text:} Aquesta tasca consisteix a generar un breu resum d'un text més llarg, com ara articles de notícies, ressenyes i informes, preservant a la vegada la informació important. Alguns models extreuen frases senceres del text d'entrada mentre que d'altres generen text completament nou. Aquesta tasca no ha sigut implementada en el projecte encara pot ser una tasca útil en una situació ideal ja que permetria eliminar tota aquella part del tiquet que no fos rellevant i així permetre que el model principal extregui els camps requerits.
    \item \textbf{Traducció automàtica:} Aquesta tasca consisteix a traduir text d'un idioma a un altre, com ara l'anglès a l'espanyol. Aquesta tasca és particularment imprecisa perquè hi ha moltes paraules o expresions que no es poden traduir d'un idioma a un altre. Pel projecte pot ser vital, ja que els models en anglès tenen un rendiment molt superior als models en català, i traduïr els tiquets, mantenint el significat, millora molt el rendiment.
    \item \textbf{Resposta a preguntes:} Aquesta tasca consisteix a trobar la resposta a una pregunta de llenguatge natural a partir d'un text o una base de coneixements determinats. Depenent del model, no és necessari subministrar un context per contestar preguntes, es pot contestar només amb l'informació que ha après. És una de les principals propostes per aquest projecte, ja que soluciona tots els problemes ingerint el tiquet com a ``context'' i permetent fer preguntes per trobar els camps buscats.
    \item \textbf{Generació de text:} Serveix per produir text coherent i fluid a partir d'una entrada determinada, com ara una instrucció o una paraula clau a substituïr. Poden ser molt versàtils, ja que generen el text més probable que bé a continuació, sent ideals per resoldre moltes tasques de \textit{NLP}. Té una aplicació directe al projecte, ja que es pot formular frases o preguntes juntament amb el tiquet a analitzar, de manera que la continuació d'aquestes sigui la resposta que es busca.
    \item \textbf{Reconeixement d'entitats:} És la tasca de trobar entitats a un text. Aquestes entitats poden ser noms de persones, llocs o organitzacions, entre altres. Cada paraula de l'entrada és assignada una etiqueta, basat en la probabilitat que la paraula pertanyi a la classe en qüestió. Tot i que normalment està dissenyat per reconeixer entitats senzilles i de poques paraules, es podria arribar a plantejar com una solució pel projecte, ja que podria aprendre a trobar els camps buscats al text com si fóssin entitats.
\end{itemize}
